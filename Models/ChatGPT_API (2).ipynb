{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1CnIa47ZAcd",
        "outputId": "7b071f83-6e55-451e-95ec-bd34e7158410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.8/dist-packages (0.27.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (3.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhqviol1YQyp"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the OpenAI API key and model ID\n",
        "openai.api_key = \"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\"\n",
        "model_engine = \"gpt-3.5-turbo\""
      ],
      "metadata": {
        "id": "L99kpXMshWP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "PRIMING_SAMPLES = 5\n",
        "PRIMING_SHUFFLE = True\n",
        "NAMED = True\n",
        "#MODEL = 'large'\n",
        "#MODEL = happy_gen\n",
        "\n",
        "#FILE_TEST_INPUT = '/content/drive/MyDrive/KG-NLG Jurassic/Experiments NLP 244/viggo_test_360_starters_pseudos.csv'\n",
        "FILE_TEST_INPUT = '/content/drive/MyDrive/AA NLP244 Project Experiments 2/Copy of viggo_test_360_starters_pseudos.csv'"
      ],
      "metadata": {
        "id": "RqGQbzV7g6Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def run_inference(test_set):\n",
        "    # generated_results = {\"mr\":[], \"pseudo\":[], \"text1\":[], \"raw_logs1\": [],\n",
        "    #                      \"text2\":[],  \"raw_logs2\": [] ,\n",
        "    #                      \"text3\": [], \"raw_logs3\": [],\n",
        "    #                      \"text4\": [],  \"raw_logs4\": [],\n",
        "    #                      \"text5\": [],  \"raw_logs5\": [],\n",
        "    #                      \"text6\": [],  \"raw_logs6\": [],\n",
        "    #                      \"text7\": [], \"raw_logs7\": [],\n",
        "    #                      \"text8\": [], \"raw_logs8\": [],\n",
        "    #                      \"text9\": [], \"raw_logs9\": [],\n",
        "    #                      \"text10\": [], \"raw_logs10\": []\n",
        "    #                      }\n",
        "  \n",
        "    generated_results = {\"index\":[], \"mr\":[], \"pseudo\":[], \"callison\":[], \"da\":[], \"text\":[], \"ref\":[]}\n",
        "    i = 0  # pointer for test example\n",
        "    j = 0  # pointer for KEYS\n",
        "    cur_key = None\n",
        "    mrs = test_set[\"mr\"].tolist()\n",
        "    das = test_set[\"da\"].tolist()\n",
        "  \n",
        "    refs = test_set[\"ref\"].tolist()\n",
        "\n",
        "    actual_test = test_set[\"test_pseudo_utt\"].tolist() # test_set[\"test_pseudo_utt\"].tolist()\n",
        "    \n",
        "     \n",
        "\n",
        " #   print(actual_test)\n",
        "    \n",
        "    for i, n in tqdm(enumerate(actual_test)):\n",
        "     #  for n in range(10): \n",
        "       #   print(i)\n",
        "        \n",
        "  \n",
        "        if \"inform\" in das:\n",
        "            with open(\"/content/drive/MyDrive/AA NLP244 Project Experiments 2/Data/Copy of inform_callison_priming.txt\") as f:\n",
        "                priming_header = f.read()\n",
        "        elif \"confirm\" in das:\n",
        "            with open(\"/content/drive/MyDrive/AA NLP244 Project Experiments 2/Data/Copy of confirm_callison_priming.txt\") as f:\n",
        "                priming_header = f.read()\n",
        "        elif \"give_opinion\" in das:\n",
        "            with open(\"/content/drive/MyDrive/AA NLP244 Project Experiments 2/Data/Copy of give opinion_callison_priming.txt\") as f:\n",
        "                priming_header = f.read()\n",
        "        elif \"recommend\" in das:\n",
        "            with open(\"/content/drive/MyDrive/AA NLP244 Project Experiments 2/Data/Copy of recommend_callison_priming.txt\") as f:\n",
        "                priming_header = f.read()\n",
        "        elif \"request\" in das:\n",
        "            with open(\"/content/drive/MyDrive/AA NLP244 Project Experiments 2/Data/Copy of request_callison_priming.txt\") as f:\n",
        "                priming_header = f.read()\n",
        "        elif \"request_attribute\" in das:\n",
        "            with open(\"/content/drive/MyDrive/AA NLP244 Project Experiments 2/Data/Copy of request attribute_callison_priming.txt\") as f:\n",
        "                priming_header = f.read()\n",
        "        elif \"request_explanation\" in das:\n",
        "            with open(\"/content/drive/MyDrive/AA NLP244 Project Experiments 2/Data/Copy of request explanation_callison_priming.txt\") as f:\n",
        "                priming_header = f.read()\n",
        "        elif \"suggest\" in das:\n",
        "            with open(\"/content/drive/MyDrive/AA NLP244 Project Experiments 2/Data/Copy of suggest_callison_priming.txt\") as f:\n",
        "                priming_header = f.read()\n",
        "        elif \"verify_attribute\" in das:\n",
        "            with open(\"/content/drive/MyDrive/AA NLP244 Project Experiments 2/Data/Copy of verify attribute_callison_priming.txt\") as f:\n",
        "                priming_header = f.read()\n",
        "        ling = \"a\"\n",
        "        if das[0] in [\"a\", \"e\", \"i\", \"o\", \"u\"]:\n",
        "            ling = \"an\"\n",
        "        mr = [mrs[i], mrs[i], mrs[i], mrs[i], mrs[i] , mrs[i] , mrs[i] , mrs[i] , mrs[i] , mrs[i]]\n",
        "        da = [das[i], das[i], das[i], das[i] , das[i], das[i] ,das[i] , das[i], das[i], das[i]]\n",
        "        curr_prompt = \"\"\n",
        "        ref = [refs[i], refs[i], refs[i], refs[i], refs[i], refs[i], refs[i], refs[i], refs[i]]\n",
        "        # typed = types[i]\n",
        "        pseudo_mr = actual_test[i]\n",
        "        priming_header = None\n",
        "        print(pseudo_mr)\n",
        "        print(\"ling:\",ling)\n",
        "\n",
        "        callison = f' Here is a text: \"{pseudo_mr}\". Here is a rewrite of the text, which is {ling} {das[i].replace(\"_\", \" \")} dialogue act: \"'\n",
        "      #  callison_new = f'Here is a text: \"{actual_test[i]}\". Rewrite it to be {ling} {das[i].replace(\"_\", \" \")} dialogue act: \"'\n",
        "    #   # paraphrase = f'Here is a text: \"{get_new_samples[get_new_samples.mr_bar == mr][\"person_starter_pseudo_mr\"].tolist()[0]}\". Here is a paraphrase of the text: \"{ref1}\"' + \"\\n\"\n",
        "      \n",
        "        #args = GENSettings(temperature=0.7, max_length=75, top_k=100, do_sample=True)\n",
        "        curr_prompt = '{}{}'.format(priming_header,callison)\n",
        "    #     print(priming_header)\n",
        "        #input_ids = tokenizer.encode(curr_prompt, return_tensors='pt')\n",
        "        #outputs = model.generate(input_ids=input_ids)\n",
        "  #generated_text = response.choices[0].text.strip()\n",
        "        #output_texts = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        output_texts = openai.ChatCompletion.create(\n",
        "        model=model_engine,\n",
        "        messages=[\n",
        "        {\"role\": \"user\", \"content\": curr_prompt[0]}       \n",
        "        ])\n",
        "        \n",
        "      #  output_text = happy_gen.generate_text(curr_prompt, args=args)\n",
        "        #output_texts = [happy_gen.generate_text(curr_prompt, args=args).text.split('\"')[0] for t in range(10)]\n",
        "       # text = output_text.text.split('\"')[0]\n",
        "        callison = [callison, callison, callison, callison, callison, callison, callison, ]\n",
        "      #  curr_prompt = \"\"\n",
        "        generated_results[\"index\"].append(i)\n",
        "        generated_results['mr'].extend(mr)\n",
        "        generated_results['pseudo'].extend(pseudo_mr)\n",
        "        generated_results['callison'].extend(callison)\n",
        "        generated_results['da'].append(da)\n",
        "        generated_results[\"ref\"].append(ref)\n",
        "        #generated_results['text'].extend(text)\n",
        "        generated_results['text'].extend(output_texts)\n",
        "        \n",
        "        \n",
        "    return generated_results\n",
        "\n",
        "\n",
        "# main function that will read the file and run inference using the function above\n",
        "  # will generate a file based on the file test output\n"
      ],
      "metadata": {
        "id": "Fluku1jdhAfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = pd.read_csv(FILE_TEST_INPUT)\n",
        "generated_results = run_inference(test_set=test_set)\n",
        "\n",
        "FILE_TEST_OUTPUT='/content/drive/MyDrive/AA NLP244 Project Experiments 2/chatgpt.csv'\n",
        "generated_df = pd.DataFrame.from_dict(generated_results)\n",
        "generated_df.to_csv(FILE_TEST_OUTPUT, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "EJQC_G5UiA5B",
        "outputId": "e3d2b189-dee9-4f77-90b1-f5fb36bfb60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Little Big Adventure average not multiplayer PlayStation\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r1it [00:01,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom Clancy's The Division M (for Mature) average role-playing, shooter, tactical third person multiplayer PlayStation, Xbox, PC Steam\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [00:02,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Super Bomberman average action, strategy Nintendo, PC not Steam not Linux not Mac\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [00:04,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M (for Mature) average first person\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r4it [00:05,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weirdest\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r5it [00:06,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Might & Magic: Heroes VI average bird view PC\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r6it [00:07,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Room excellent indie, point-and-click, puzzle PC\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r7it [00:09,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom Clancy's The Division average role-playing, shooter, tactical not Mac\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r8it [00:10,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tony Hawk's Pro Skater 3 2001 sport\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:12,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mirror's Edge Catalyst poor not Steam\n",
            "ling: a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-816fc07bb708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILE_TEST_INPUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerated_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mFILE_TEST_OUTPUT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/AA NLP244 Project Experiments 2/chatgpt.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgenerated_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6271a57bff0b>\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(test_set)\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;31m#generated_text = response.choices[0].text.strip()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m#output_texts = tokenizer.decode(outputs[0], skip_special_tokens=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         output_texts = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_engine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         )\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             return (\n\u001b[0;32m--> 619\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    680\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-4VEsknR20GAIe86O016hPqMo on requests per min. Limit: 20 / min. Current: 30 / min. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Sm8-j_LJYxzT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}