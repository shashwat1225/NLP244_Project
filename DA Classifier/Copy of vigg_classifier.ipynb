{"cells":[{"cell_type":"code","source":["!pip install regex pandarallel datasets torch transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-JLTYKa4c11J","executionInfo":{"status":"ok","timestamp":1679786752327,"user_tz":420,"elapsed":24485,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"5fa9ded2-c44d-439f-dd73-e800df8efbb8"},"id":"-JLTYKa4c11J","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (2022.10.31)\n","Collecting pandarallel\n","  Downloading pandarallel-1.6.4.tar.gz (12 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting datasets\n","  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n","Collecting transformers\n","  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dill>=0.3.1\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1 in /usr/local/lib/python3.9/dist-packages (from pandarallel) (1.4.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from pandarallel) (5.9.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting huggingface-hub<1.0.0,>=0.2.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1->pandarallel) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1->pandarallel) (2022.7.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1->pandarallel) (1.16.0)\n","Building wheels for collected packages: pandarallel\n","  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pandarallel: filename=pandarallel-1.6.4-py3-none-any.whl size=16677 sha256=af3131846ffb448776f5c91960e0a204e6e458cc348494b3ac6b5e1d4b2a8d21\n","  Stored in directory: /root/.cache/pip/wheels/41/01/29/deaa71fe596f8d857e57c4fb388db8861e23e6ed0b03204dcb\n","Successfully built pandarallel\n","Installing collected packages: tokenizers, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, pandarallel, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.3 multidict-6.0.4 multiprocess-0.70.14 pandarallel-1.6.4 responses-0.18.0 tokenizers-0.13.2 transformers-4.27.3 xxhash-3.2.0 yarl-1.8.2\n"]}]},{"cell_type":"code","execution_count":15,"id":"b2ec3044-b5f9-4b26-a710-1d70e6fbfebf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2ec3044-b5f9-4b26-a710-1d70e6fbfebf","executionInfo":{"status":"ok","timestamp":1679786950703,"user_tz":420,"elapsed":109,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"bdd4a618-dac1-4d9b-f7f6-d5335ecf5cc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO: Pandarallel will run on 1 workers.\n","INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"]}],"source":["\n","import os\n","\n","#os.environ['HF_HOME'] = '/data/users/ugarg/hf/hf_cache/'\n","#os.environ['TRANSFORMERS_CACHE'] = '/data/users/ugarg/hf/hf_cache/'\n","os.environ['CUDA_VISIBLE_DEVICES']='1'\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","from pandarallel import pandarallel\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","pandarallel.initialize(progress_bar=False)\n","from torch.nn.functional import one_hot\n","\n","\n","from collections import Counter\n","\n","from datasets import load_dataset, Dataset, DatasetDict\n","import pandas as pd\n","import torch\n","import numpy as np\n","import random\n","from transformers import AdamW, AutoTokenizer,  AutoModel\n","from torch.nn.functional import one_hot\n","from collections import Counter\n","def seed_everything(seed: int):\n","    \n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","    \n","seed_everything(42)\n","import joblib\n","\n"]},{"cell_type":"code","execution_count":16,"id":"735983dd-b32a-48b8-85b8-d8c4a821343a","metadata":{"id":"735983dd-b32a-48b8-85b8-d8c4a821343a","executionInfo":{"status":"ok","timestamp":1679786953647,"user_tz":420,"elapsed":3,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}}},"outputs":[],"source":["def format_table(df):\n","    df['labels'] = df['mr'].apply(lambda x: x.split('(',1)[0])\n","    df = df.drop('mr', 1)\n","    return df"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"879Zrguodq8N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679786847192,"user_tz":420,"elapsed":15645,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"20ca24e6-8756-4c22-f501-aeea404ee3c2"},"id":"879Zrguodq8N","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":18,"id":"7349ba9f-ec83-434e-83a0-fef376f235f6","metadata":{"id":"7349ba9f-ec83-434e-83a0-fef376f235f6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679786995469,"user_tz":420,"elapsed":1016,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"b9e31350-2264-4c6e-c3fb-d0dd4ff912f0"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-203a5f988e2c>:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  df = df.drop('mr', 1)\n","<ipython-input-16-203a5f988e2c>:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  df = df.drop('mr', 1)\n","<ipython-input-16-203a5f988e2c>:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  df = df.drop('mr', 1)\n"]}],"source":["train = pd.read_csv('/content/drive/MyDrive/AA NLP244 Project Experiments/DA Classifier/viggo-train.csv')\n","val = pd.read_csv('/content/drive/MyDrive/AA NLP244 Project Experiments/DA Classifier/viggo-valid.csv')\n","test = pd.read_csv('/content/drive/MyDrive/AA NLP244 Project Experiments/DA Classifier/viggo-test.csv')\n","train, test, valid = format_table(train), format_table(test), format_table(val)"]},{"cell_type":"code","execution_count":null,"id":"15a09a76-35f5-4401-bba9-6ac851659f46","metadata":{"id":"15a09a76-35f5-4401-bba9-6ac851659f46"},"outputs":[],"source":["# train.info()"]},{"cell_type":"code","execution_count":null,"id":"823353db-7602-4aed-921d-4a2ec15f9634","metadata":{"id":"823353db-7602-4aed-921d-4a2ec15f9634"},"outputs":[],"source":["# train.labels.value_counts()"]},{"cell_type":"code","execution_count":null,"id":"1375bf52-4c0b-41d6-8ca0-fb095450ead9","metadata":{"id":"1375bf52-4c0b-41d6-8ca0-fb095450ead9"},"outputs":[],"source":["# def show_sample(df, num_rows):\n","#     for i in range(num_rows):\n","#         idx = np.random.choice(len(df))\n","#         ref = df.loc[idx]['ref']\n","#         da = df.loc[idx]['labels']\n","#         #domain = df.loc[idx]['domain']\n","#         print('-'*50)\n","#         print(ref)\n","#         print(da)\n","#         #print(domain)\n","#         print('-'*50)\n","\n","# print(\"\\nSample Data: \")\n","# print(show_sample(train, 3))\n"]},{"cell_type":"code","execution_count":null,"id":"24fdc679-175b-467c-9844-cb66ee094c78","metadata":{"id":"24fdc679-175b-467c-9844-cb66ee094c78"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"id":"d5a78e28-3c15-40b3-87ef-832cb0a3f5e8","metadata":{"id":"d5a78e28-3c15-40b3-87ef-832cb0a3f5e8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679786894074,"user_tz":420,"elapsed":317,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"c55bb55f-15f4-4cb0-8ed2-77e0aaf7b9b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading Tokenizer: bert-base-cased ...\n"]}],"source":["\n","DROPOUT = 0.3\n","LEARNING_RATE = 1e-4\n","NUM_EPOCHS = 50\n","MAX_LENGTH = 100\n","BATCH_SIZE = 64\n","EARLY_STOPPING_LIMIT = 5\n","DEVICE = 'cpu'\n","\n","save_checkpoint_path = '/content/drive/MyDrive/AA NLP244 Project Experiments/DA Classifier/Copy of best_model.pt'\n","resume_from_checkpoint = True\n","\n","\n","\n","\n","##################\n","#   tokenizer\n","##################\n","\n","\n","from transformers import AutoTokenizer\n","\n","model_checkpoint = \"bert-base-cased\"\n","print(f'\\nLoading Tokenizer: {model_checkpoint} ...')\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"769c64b8-9dad-4836-b449-5ed89643b2d7","metadata":{"id":"769c64b8-9dad-4836-b449-5ed89643b2d7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":19,"id":"84b8a02c-27a6-4bc1-903c-03806aa957da","metadata":{"id":"84b8a02c-27a6-4bc1-903c-03806aa957da","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679787003941,"user_tz":420,"elapsed":223,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"9833fb56-82c0-4429-b848-009271ed8bac"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Creating Pytorch Datasets and Loaders...\n","\n","Keeping max length: 100\n","Batch Size: 64\n"]}],"source":["\n","\n","####################################\n","#   Create Pytorch Datasets\n","####################################\n","\n","class TrainDataset:\n","    def __init__(self, train, tokenizer, max_length):\n","        self.train = train\n","        self.tokenizer = tokenizer\n","        self.source_column = 'ref'\n","        self.label_column = 'labels'\n","        self.max_length = max_length\n","        self.build_vocab()\n","    \n","    def __len__(self):\n","        return len(self.train)\n","    \n","    def build_vocab(self,):\n","         #get vocab\n","        self.stoi = {'request': 0,\n","                     'inform': 1,\n","                     'suggest': 2,\n","                     'request_attribute': 3,\n","                     'verify_attribute': 4,\n","                     'confirm': 5,\n","                     'give_opinion': 6,\n","                     'request_explanation': 7,\n","                     'recommend': 8}\n","        self.itos = {j:i for i,j in self.stoi.items()}\n","        \n","        self.num_class = len(self.itos)\n","        \n","        \n","    def __getitem__(self, idx):\n","        tokenized_source_text = self.tokenizer(self.train[self.source_column].loc[idx], truncation=True, padding=True, max_length = self.max_length)\n","        label = self.stoi[self.train[self.label_column][idx]]\n","        #one_hot_labels = one_hot(torch.tensor(label), num_classes = self.num_class)\n","        tokenized_source_text['label'] = label\n","        return tokenized_source_text\n","\n","    \n","class ValDataset:\n","    def __init__(self, train_dataset, val, tokenizer, max_length):\n","        self.stoi = train_dataset.stoi\n","        self.itos = train_dataset.itos\n","        self.num_class = len(self.stoi)\n","        self.val = val\n","        self.tokenizer = tokenizer\n","        self.source_column = 'ref'\n","        self.label_column = 'labels'\n","        self.max_length = max_length\n","    def __len__(self):\n","        return len(self.val)\n","\n","        \n","    def __getitem__(self, idx):\n","        tokenized_source_text = self.tokenizer(self.val[self.source_column].loc[idx], truncation=True, padding=True, max_length = self.max_length)\n","        label = self.stoi[self.val[self.label_column][idx]]\n","        tokenized_source_text['label'] = label\n","        return tokenized_source_text\n","        \n","        \n","print(f'\\nCreating Pytorch Datasets and Loaders...')\n","\n","print(f'\\nKeeping max length: {MAX_LENGTH}\\nBatch Size: {BATCH_SIZE}')\n","\n"]},{"cell_type":"code","execution_count":20,"id":"f78ca1c0-6a07-4609-a41e-43a2a04b8c90","metadata":{"id":"f78ca1c0-6a07-4609-a41e-43a2a04b8c90","executionInfo":{"status":"ok","timestamp":1679787005878,"user_tz":420,"elapsed":2,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}}},"outputs":[],"source":["\n","train_dataset = TrainDataset(train, tokenizer, max_length = MAX_LENGTH)\n","val_dataset = ValDataset(train_dataset, val, tokenizer, max_length = MAX_LENGTH)\n","test_dataset = ValDataset(train_dataset, test, tokenizer, max_length = MAX_LENGTH)\n"]},{"cell_type":"code","execution_count":null,"id":"0a52fca4-07de-41b7-8371-73cdef0c6b3c","metadata":{"id":"0a52fca4-07de-41b7-8371-73cdef0c6b3c"},"outputs":[],"source":["# train_dataset.itos"]},{"cell_type":"code","execution_count":null,"id":"6e8161dc-e1ca-40b0-9a07-c1cebc8209e4","metadata":{"id":"6e8161dc-e1ca-40b0-9a07-c1cebc8209e4"},"outputs":[],"source":["# train_dataset.stoi"]},{"cell_type":"code","execution_count":21,"id":"2b4de6d6-dfeb-4d1e-bbe7-9c8c6d4fafb8","metadata":{"id":"2b4de6d6-dfeb-4d1e-bbe7-9c8c6d4fafb8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679787008706,"user_tz":420,"elapsed":288,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"8b879940-749a-4776-ca25-1970568a6764"},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Shape of batch:  torch.Size([64, 73])\n"]}],"source":["\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    shuffle=True,\n","    collate_fn = data_collator,\n","    batch_size=BATCH_SIZE,\n",")\n","val_dataloader = DataLoader(\n","    val_dataset, collate_fn = data_collator, batch_size=BATCH_SIZE\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset, collate_fn = data_collator, batch_size=BATCH_SIZE\n",")\n","\n","print(\"\\nShape of batch: \", next(iter(train_dataloader))['input_ids'].shape)"]},{"cell_type":"code","execution_count":22,"id":"c6786bb5-40e6-46b6-aba8-4c97f6debbf0","metadata":{"id":"c6786bb5-40e6-46b6-aba8-4c97f6debbf0","executionInfo":{"status":"ok","timestamp":1679787010203,"user_tz":420,"elapsed":115,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}}},"outputs":[],"source":["\n","\n","####################################\n","#   Create Model Class\n","####################################\n","\n","\n","class Model(torch.nn.Module):\n","    def __init__(self, model_checkpoint, train_dataset, dropout, device):\n","        super().__init__()\n","        self.base = AutoModel.from_pretrained(\n","                model_checkpoint,\n","                #id2label=id2label,\n","                #label2id=label2id,\n","                )\n","        \"\"\"\n","        Uncomment if want to use adapter\n","        #         self.base.add_adapter('adap')\n","\n","        #         self.base.train_adapter(\"adap\")\n","        #         self.base.set_active_adapters(\"adap\")\n","\n","        Uncomment if want to unfreeze specific layers\n","                # c = 0\n","                # for name, param in self.base.named_parameters():\n","                #     c+=1\n","                #     if c <= 100:\n","                #         param.requires_grad = False\n","\n","        \"\"\"\n","        self.stoi = train_dataset.stoi\n","        self.itos = train_dataset.itos\n","        self.device = device\n","        self.fc = torch.nn.Linear(self.base.config.hidden_size, len(self.stoi))\n","        \n","        self.dropout = torch.nn.Dropout(dropout)\n","        self.loss_fn = torch.nn.CrossEntropyLoss()    \n","        \n","        \n","        \n","    \n","    def forward(self, batch, mode = 'Train'):\n","        batch.to(self.device)\n","        self.base.to(self.device)\n","        self.fc.to(self.device)\n","        _ , pools = self.base(input_ids=batch['input_ids'],\n","                      attention_mask=batch['attention_mask'],\n","                      token_type_ids=batch['token_type_ids'],\n","                       return_dict=False)\n","        # print(hidden.shape)\n","        # print(pools.shape)\n","\n","        if mode == 'Train':\n","            \n","            out = self.fc(self.dropout(pools))\n","\n","            actual = batch['labels']\n","            loss = self.loss_fn(out, actual)\n","\n","            return loss, out, actual\n","        else:\n","            \n","            out = self.fc(self.dropout(pools))\n","            return out\n","\n","\n"]},{"cell_type":"code","execution_count":23,"id":"e60f9f14-332d-4ebe-8619-4fcb71db9857","metadata":{"id":"e60f9f14-332d-4ebe-8619-4fcb71db9857","colab":{"base_uri":"https://localhost:8080/","height":191,"referenced_widgets":["3b49d3413ec344488a056854045f62f8","0c656ce6915f42f5be9104709d75954e","4cc837a2eb5d431c8808ff33ed4fcb67","11849b4f84f344b99b52dffaf53c81fc","fcf5589714fd4cce85af3a5c5dd26f00","8af565791afd4645922bda5e017e7d17","921d9804aaaf467dbd6ec9deb85f0077","a314bbe505d3439e8dba60608c01e3a1","2196568f99694a44912258605d57d4ed","975b2c13c15f44b58727f769b42dce45","ce8f34cfd39b43eeb871921587ccc378"]},"executionInfo":{"status":"ok","timestamp":1679787016001,"user_tz":420,"elapsed":4109,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"71b1d689-a88f-4ecb-ff5c-ffdb8ff2cf6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Using device cpu for training...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b49d3413ec344488a056854045f62f8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["\n","Creating optimizer with Learning Rate: 0.0001\n"]}],"source":["\n","####################################\n","#   Load Model\n","####################################\n","\n","print(f'\\nUsing device {DEVICE} for training...')\n","\n","model = Model(model_checkpoint, train_dataset, DROPOUT, DEVICE)\n","model.to(DEVICE)\n","\n","####################################\n","#   Create Optimizer\n","####################################\n","print(f'\\nCreating optimizer with Learning Rate: {LEARNING_RATE}')\n","\n","from torch.optim import AdamW, Adam\n","\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"code","execution_count":null,"id":"56a66b8e-b138-456b-b2d2-a80b20b99406","metadata":{"id":"56a66b8e-b138-456b-b2d2-a80b20b99406"},"outputs":[],"source":["# # In[8]:\n","\n","\n","\n","\n","# from transformers import get_scheduler, get_cosine_with_hard_restarts_schedule_with_warmup\n","# print(f'\\nTraining for {NUM_EPOCHS} epochs with early stopping set to {EARLY_STOPPING_LIMIT}\\n\\n')\n","\n","# num_train_epochs = NUM_EPOCHS\n","# num_update_steps_per_epoch = len(train_dataloader)\n","# num_training_steps = num_train_epochs * num_update_steps_per_epoch\n","\n","# # lr_scheduler = get_scheduler(\n","# #     \"linear\",\n","# #     optimizer=optimizer,\n","# #     num_warmup_steps=500,\n","# #     num_training_steps=num_training_steps,\n","# # )\n","# lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n","#     optimizer=optimizer,\n","#     num_warmup_steps=500,\n","#     num_training_steps=num_training_steps,\n","# )\n","\n","\n","# # In[9]:\n","\n","\n","# ####################################\n","# #   Create Checkpoint Builder\n","# ####################################\n","\n","# def checkpoint_builder(model, optimizer, epoch, train_dataset, save_checkpoint_path):\n","#     \"\"\"\n","#     desc:\n","#         creates checkpoint for the model\n","#     params:\n","#         model: trained model\n","#         optimizer\n","#         criterion: loss fn\n","#         epoch: checkpoint epoch\n","#         model_params: hyperparams of model\n","#         data_module: object of DataModule => contains the datasets and vocabulary\n","#         save_checkpoint_path: path to save checkpoint. Include checkpoint name\n","#     \"\"\"\n","    \n","#     torch.save({'model_state_dict': model.state_dict(),\n","#                         'optim_state_dict':optimizer.state_dict(),\n","                        \n","#                         'epoch': epoch, \n","                        \n","#                         'source_stoi': train_dataset.stoi,\n","#                         'source_itos': train_dataset.itos,\n","#                        }, save_checkpoint_path)\n","\n","\n","# # In[10]:\n","\n","\n","\n","\n","# from tqdm.auto import tqdm\n","# import torch\n","\n","# progress_bar = tqdm(range(num_training_steps))\n","# early_stopping_counter = 0\n","# early_stopping_limit = EARLY_STOPPING_LIMIT\n","# best_valid_loss = float('inf')\n","# for epoch in range(num_train_epochs):\n","#     train_loss=0\n","#     valid_loss =0\n","    \n","\n","    \n","#     print(f\"[Epoch {epoch} / {num_train_epochs}]\")\n","#     # Training\n","#     model.train()\n","#     for batch_idx, batch in enumerate(train_dataloader):\n","#         loss, out, actual = model(batch)\n","#         #loss = outputs.loss\n","#         loss.backward(loss)#, retain_graph = True)\n","\n","#         optimizer.step()\n","#         lr_scheduler.step()\n","#         optimizer.zero_grad()\n","#         train_loss+= ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n","#         progress_bar.update(1)\n","#         progress_bar.set_postfix(loss = train_loss)\n","        \n","    \n","    \n","#     # Evaluation\n","#     model.eval()\n","#     for batch in val_dataloader:\n","#         with torch.no_grad():\n","#             loss, out, actual = model(batch)\n","\n","\n","#         labels = actual\n","#         predictions = out\n","        \n","\n","#         valid_loss+= ((1 / (batch_idx + 1)) * (loss.data.item() - valid_loss))\n","        \n","\n","#     print('\\nEpoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","#             epoch, \n","#             train_loss,\n","#             valid_loss\n","#             ))\n","\n","#     #early stopping, checkpointing\n","#     if valid_loss < best_valid_loss:\n","#         early_stopping_counter = 0\n","#         best_valid_loss = valid_loss\n","#         #create checkpoint\n","#         print(\"Loss improved saving checkpoint... \")\n","#         checkpoint_builder(model, optimizer, epoch, train_dataset, save_checkpoint_path)\n","\n","#     else:\n","#         early_stopping_counter += 1\n","#         if early_stopping_counter >= early_stopping_limit:\n","#             print(f'\\nLoss did not reduce for last {early_stopping_counter} epochs. Stopped training..')\n","#             break\n","\n","\n","# # In[ ]:\n","\n","\n","\n"]},{"cell_type":"code","execution_count":24,"id":"2dd85158-f8ab-4e19-86a4-090cd53d7220","metadata":{"id":"2dd85158-f8ab-4e19-86a4-090cd53d7220","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679787023711,"user_tz":420,"elapsed":2807,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"a9d6a09a-5cfa-4cc5-fce1-30161f9b520e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Checkpoint ...\n","Setting Models state to checkpoint...\n","Model state set.\n","Optimizer state set.\n"]}],"source":["\n","\n","# ### Testing\n","\n","# In[ ]:\n","\n","\n","\n","if resume_from_checkpoint:\n","        print('Loading Checkpoint ...')\n","        checkpoint = torch.load(save_checkpoint_path, map_location=DEVICE)\n","        source_stoi = checkpoint['source_stoi']\n","        source_itos = checkpoint['source_itos']                \n","        print('Setting Models state to checkpoint...')\n","        #assert checkpoint['model_params'] == model_params\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        print('Model state set.')\n","        optimizer.load_state_dict(checkpoint['optim_state_dict'])\n","        print('Optimizer state set.')\n"]},{"cell_type":"code","execution_count":null,"id":"1303b24d-57fa-4282-8fce-26f31be7bb73","metadata":{"id":"1303b24d-57fa-4282-8fce-26f31be7bb73"},"outputs":[],"source":["# source_stoi"]},{"cell_type":"code","execution_count":null,"id":"f99c09bf-19b2-49c1-b0b4-3c91128182a7","metadata":{"id":"f99c09bf-19b2-49c1-b0b4-3c91128182a7"},"outputs":[],"source":["# list(source_stoi.keys())"]},{"cell_type":"code","execution_count":null,"id":"9e64a993-7a1f-4c3c-8c15-69679578f694","metadata":{"id":"9e64a993-7a1f-4c3c-8c15-69679578f694"},"outputs":[],"source":["# train_dataset.stoi"]},{"cell_type":"code","execution_count":null,"id":"48fe1c11-297b-4d7e-84f0-56fe3f95ab72","metadata":{"id":"48fe1c11-297b-4d7e-84f0-56fe3f95ab72"},"outputs":[],"source":["# train_dataset[1]"]},{"cell_type":"code","execution_count":null,"id":"56e045bd-0630-48c9-a1aa-58d3ea859737","metadata":{"id":"56e045bd-0630-48c9-a1aa-58d3ea859737"},"outputs":[],"source":["\n","\n","\n","\n","# #prints the examples that were wrongly predicted\n","# #prints the Accuracy and returns it\n","# #print the classification report\n","# def get_accuracy(dataloader, print_errors = False):\n","#     acc = []\n","#     preds = []\n","#     acts = []\n","#     for batch_idx, batch in enumerate(dataloader):\n","        \n","#         _, out, actual = model(batch)\n","#         out = torch.argmax(out, axis = 1)\n","#         out = out.cpu().numpy()\n","#         actual = actual.cpu().numpy()\n","        \n","#         acc.append([i == j for i, j in zip(out,actual)])\n","#         preds.append(out.tolist())\n","#         acts.append(actual.tolist())\n","#         del(out, actual,_,batch)\n","    \n","#     print(f'Accuracy: {np.mean(sum(acc,[]))}')\n","    \n","#     from sklearn.metrics import classification_report\n","\n","#     label_names = list(source_stoi.keys())\n","#     print(classification_report(sum(acts,[]), sum(preds,[]),target_names=label_names))\n","#     from sklearn.metrics import confusion_matrix\n","#     from sklearn.metrics import ConfusionMatrixDisplay\n","#     dis = ConfusionMatrixDisplay(confusion_matrix(sum(acts,[]), sum(preds,[])), display_labels=label_names)\n","#     dis.plot(xticks_rotation=90)\n","#     plt.show()\n","    \n","\n","# # In[15]:\n","# #model.eval()\n","# # model.to(DEVICE)\n","# print('\\nValidation Results:')\n","# get_accuracy(val_dataloader, print_errors = True)\n","# print('\\nTest Results:')\n","# get_accuracy(test_dataloader)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["prompt_df = pd.read_csv('/content/drive/MyDrive/AA NLP244 Project Experiments/Evaluation/ChatGPT/NLP244_TV_chatgpt_90_responses.csv')\n","\n","prompt_df = prompt_df[['mr','pseudo','callison', 'da', 'text' ]].dropna().reset_index(drop=True)\n","\n","prompt_df.columns = ['mr','pseudo','callison', 'da', 'text']\n"],"metadata":{"id":"3Bw62fvyrSkG","executionInfo":{"status":"ok","timestamp":1679787049808,"user_tz":420,"elapsed":291,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}}},"id":"3Bw62fvyrSkG","execution_count":25,"outputs":[]},{"cell_type":"code","source":["def predict(col):\n","    df = prompt_df.copy()\n","    tokenized_data = tokenizer(col)\n","    tokenized_data = data_collator(tokenized_data)\n","    out = model(tokenized_data, mode = 'Test')\n","    probs = torch.nn.functional.softmax(out, dim = 1).detach().cpu().numpy()\n","    pred = out.argmax(dim = 1).cpu().numpy()\n","    pred = [train_dataset.itos[i] for i in pred]\n","    probs = [i.max() for i in probs]\n","    return pred, probs\n","\n","#prompt_df = p8redict('candidate')\n","preds = []\n","probs = []\n","for i in range(len(prompt_df)): \n","  print(i)\n","  pred, prob = predict([prompt_df[\"text\"][i]])\n","  preds.extend(pred)\n","  probs.extend(prob)\n","\n","prompt_df[\"pred\"] = preds\n","prompt_df[\"probs\"] = probs\n","\n","prompt_df.to_csv('/content/drive/MyDrive/AA NLP244 Project Experiments/Evaluation/ChatGPT/NLP244_TV_chatgpt_eval-DA.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpdPUoKX-zEY","executionInfo":{"status":"ok","timestamp":1679787095608,"user_tz":420,"elapsed":17841,"user":{"displayName":"Shashwat Pandey","userId":"04416711838524522488"}},"outputId":"13051bb2-b058-4c88-f715-5fe5a0a300dc"},"id":"PpdPUoKX-zEY","execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n"]}]},{"cell_type":"code","source":["from datasets import Dataset as D\n","def tokenize(values): \n","   input_ids = tokenizer(values[\"text\"])\n","   return input_ids\n","\n","def predict(df, col):\n","    dataset = D.from_pandas(df[])\n","    #print(dataset)\n","    tokenized_data = dataset.map(tokenize, batched=True, batch_size=64)\n","    # print(type(tokenized_data)) \n","    # print(tokenized_data)\n","    dicts = {\"input_ids\":tokenized_data[\"input_ids\"], \"token_type_ids\":tokenized_data[\"token_type_ids\"]}\n","    tokenized_data = data_collator(dicts)\n","    out = model(tokenized_data, mode = 'Test')\n","    probs = torch.nn.functional.softmax(out, dim = 1).detach().cpu().numpy()\n","    pred = out.argmax(dim = 1).cpu().numpy()\n","    pred = [train_dataset.itos[i] for i in pred]\n","    probs = [i.max() for i in probs]\n","\n","    # df[f'{col}_pred'] = pred\n","    # df[f'{col}_prob'] = probs\n","    # for i in range(len(df)):\n","    #     df.loc[i,f'{col}_pred'] = pred[i]\n","    #     df.loc[i,f'{col}_prob'] = probs[i]\n","    return pred, probs\n","\n","#prompt_df = p8redict('candidate')\n","preds = []\n","probs = []\n","for i in range(len(prompt_df)): \n","  print(prompt_df[\"text\"][i])\n","  pred, prob = predict(prompt_df[\"text\"][i])\n","  preds.append(pred)\n","  probs.append(prob)\n","\n","df[\"pred\"] = preds\n","df[\"probs\"] = probs\n","#prompt_df['actual'] = prompt_df['mr'].apply(lambda x: x.split('|')[0].split('=')[0].strip())\n","\n","#print(prompt_df.apply(lambda x: 1 if x['candidate_pred'] == x['da'] else 0,axis=1).value_counts())\n","\n","prompt_df.to_csv('/content/drive/MyDrive/KG-NLG Jurassic/Experiments NLP 244/kg_outputs/jurassic_eval-DA.csv')"],"metadata":{"id":"FwEKWwXblIw-","colab":{"base_uri":"https://localhost:8080/","height":514},"executionInfo":{"status":"error","timestamp":1679533159507,"user_tz":420,"elapsed":1843,"user":{"displayName":"Angela Ramirez","userId":"10429329820919628813"}},"outputId":"85267a8e-45a9-48d9-d201-d27d44a5df80"},"id":"FwEKWwXblIw-","execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3629\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-7de4383daece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3631\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3632\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3633\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0"]}]},{"cell_type":"code","source":[],"metadata":{"id":"G5mr8na1tx_1"},"id":"G5mr8na1tx_1","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"1d0e498d-3206-4d2a-a699-75bc6c00d785","metadata":{"id":"1d0e498d-3206-4d2a-a699-75bc6c00d785"},"outputs":[],"source":["prompt_df = pd.read_csv('/content/drive/MyDrive/KG-NLG Jurassic/Experiments NLP 244/kg_outputs/chatgpt_eval - chatgpt_eval.csv')\n","\n","prompt_df = prompt_df[['index', 'mr','pseudo','callison', 'da', 'text' ]].dropna().reset_index(drop=True)\n","\n","prompt_df.columns = ['index', 'mr','pseudo','callison', 'da', 'text']\n","\n","def predict(col):\n","    df = prompt_df.copy()\n","    tokenized_data = tokenizer(df[col].values.tolist())\n","    print(tokenized_data)\n","    print(type(tokenized_data))\n","    tokenized_data = data_collator(tokenized_data)\n","    print(type(tokenized_data))\n","    out = model(tokenized_data, mode = 'Test')\n","    probs = torch.nn.functional.softmax(out, dim = 1).detach().cpu().numpy()\n","    pred = out.argmax(dim = 1).cpu().numpy()\n","    pred = [train_dataset.itos[i] for i in pred]\n","    probs = [i.max() for i in probs]\n","\n","    df[f'{col}_pred'] = 1\n","    df[f'{col}_prob'] = 1\n","    for i in range(len(df)):\n","        df.loc[i,f'{col}_pred'] = pred[i]\n","        df.loc[i,f'{col}_prob'] = probs[i]\n","    return df\n","\n","#prompt_df = predict('candidate')\n","prompt_df = predict('text')\n","#prompt_df['actual'] = prompt_df['mr'].apply(lambda x: x.split('|')[0].split('=')[0].strip())\n","\n","#print(prompt_df.apply(lambda x: 1 if x['candidate_pred'] == x['da'] else 0,axis=1).value_counts())\n","\n","prompt_df.to_csv('/content/drive/MyDrive/KG-NLG Jurassic/Experiments NLP 244/kg_outputs/chatgpt_eval-DA.csv')"]},{"cell_type":"code","execution_count":null,"id":"5be9ce84-dfed-4f8e-9059-b72fd4b099fe","metadata":{"id":"5be9ce84-dfed-4f8e-9059-b72fd4b099fe"},"outputs":[],"source":["# prompt_df['check'] = prompt_df.candidate.apply(lambda x: 1 if 'did' in x else 0)"]},{"cell_type":"code","execution_count":null,"id":"c0c4b439-bef8-46d3-8011-342310f2eacc","metadata":{"id":"c0c4b439-bef8-46d3-8011-342310f2eacc"},"outputs":[],"source":["# prompt_df[prompt_df.check==1]"]},{"cell_type":"code","execution_count":null,"id":"051f507c-a210-4305-92f2-f086136105e6","metadata":{"id":"051f507c-a210-4305-92f2-f086136105e6"},"outputs":[],"source":["# prompt_df = pd.read_csv('../test_files/viggo-test-specific-10examples-new-format-1-100 - viggo-test-specific-10examples-1-100.csv')\n","\n","# prompt_df = prompt_df[['MR', 'Candidate','best ref',' DA (perfect  3, 2, 1 means not really at all)']].dropna().reset_index(drop=True)\n","\n","# prompt_df.columns = ['mr', 'candidate', 'best_ref', 'related']\n","\n","# def predict(col):\n","#     df = prompt_df.copy()\n","#     tokenized_data = tokenizer(df[col].values.tolist())\n","#     tokenized_data = data_collator(tokenized_data)\n","\n","#     out = model(tokenized_data, mode = 'Test')\n","#     probs = torch.nn.functional.softmax(out, dim = 1).detach().cpu().numpy()\n","#     pred = out.argmax(dim = 1).cpu().numpy()\n","#     pred = [train_dataset.itos[i] for i in pred]\n","#     probs = [i.max() for i in probs]\n","\n","#     df[f'{col}_pred'] = 1\n","#     df[f'{col}_prob'] = 1\n","#     for i in range(len(df)):\n","#         df.loc[i,f'{col}_pred'] = pred[i]\n","#         df.loc[i,f'{col}_prob'] = probs[i]\n","#     return df\n","\n","# prompt_df = predict('candidate')\n","# prompt_df = predict('best_ref')\n","# prompt_df['actual'] = prompt_df['mr'].apply(lambda x: x.split('|')[0].split('=')[0].split(':')[1].strip())\n","# print(prompt_df.apply(lambda x: 1 if x['candidate_pred'] == x['actual'] else 0,axis=1).value_counts())\n","# prompt_df.to_csv('viggo-test-specific-10examples-new-format-1-100 - viggo-test-specific-10examples-1-100.csv')"]},{"cell_type":"code","execution_count":null,"id":"170a3ab1-db48-4c3f-a76e-f26f9bc7d569","metadata":{"id":"170a3ab1-db48-4c3f-a76e-f26f9bc7d569"},"outputs":[],"source":["# prompt_df.related.value_counts()"]},{"cell_type":"code","execution_count":null,"id":"cad99142-2228-40dc-8d6f-40d43be64f58","metadata":{"id":"cad99142-2228-40dc-8d6f-40d43be64f58"},"outputs":[],"source":["# print(prompt_df.apply(lambda x: 1 if x['candidate_pred'] == x['actual'] else 0,axis=1).value_counts())"]},{"cell_type":"code","execution_count":null,"id":"f1912815-1a42-49d2-82dc-0adf7d6018e5","metadata":{"id":"f1912815-1a42-49d2-82dc-0adf7d6018e5"},"outputs":[],"source":["# print(prompt_df.apply(lambda x: 1 if x['candidate_pred'] == x['actual'] and x['related']!=3 else 0,axis=1).value_counts())"]},{"cell_type":"code","execution_count":null,"id":"f661c8fe-fc1d-4e57-a8be-17beb1af592a","metadata":{"id":"f661c8fe-fc1d-4e57-a8be-17beb1af592a"},"outputs":[],"source":["# prompt_df = pd.read_csv('../test_files/viggo_test_5_shuffle_output - cc-clean.csv')\n","\n","# prompt_df = prompt_df[['mr', 'generated']].reset_index(drop=True)\n","\n","# prompt_df.columns = ['mr', 'candidate']\n","\n","# def predict(col):\n","#     df = prompt_df.copy()\n","#     tokenized_data = tokenizer(df[col].values.tolist())\n","#     tokenized_data = data_collator(tokenized_data)\n","\n","#     out = model(tokenized_data, mode = 'Test')\n","#     probs = torch.nn.functional.softmax(out, dim = 1).detach().cpu().numpy()\n","#     pred = out.argmax(dim = 1).cpu().numpy()\n","#     pred = [train_dataset.itos[i] for i in pred]\n","#     probs = [i.max() for i in probs]\n","\n","#     df[f'{col}_pred'] = 1\n","#     df[f'{col}_prob'] = 1\n","#     for i in range(len(df)):\n","#         df.loc[i,f'{col}_pred'] = pred[i]\n","#         df.loc[i,f'{col}_prob'] = probs[i]\n","#     return df\n","\n","# prompt_df = predict('candidate')\n","# #prompt_df = predict('best_ref')\n","# prompt_df['actual'] = prompt_df['mr'].apply(lambda x: x.split('|')[0].split('=')[0].strip())\n","# print(prompt_df.apply(lambda x: 1 if x['candidate_pred'] == x['actual'] else 0,axis=1).value_counts())\n","# prompt_df.to_csv('viggo_test_5_shuffle_output - cc-clean.csv')"]},{"cell_type":"code","execution_count":null,"id":"42d391e2-850b-4fb0-afb7-17986b87433c","metadata":{"id":"42d391e2-850b-4fb0-afb7-17986b87433c"},"outputs":[],"source":["# prompt_df.apply(lambda x: 1 if x['candidate_pred'] == x['actual'] else 0, axis=1).value_counts()"]},{"cell_type":"code","execution_count":null,"id":"ab0c2c73-98b2-4516-8a82-1b258b9d9bcf","metadata":{"id":"ab0c2c73-98b2-4516-8a82-1b258b9d9bcf"},"outputs":[],"source":["# prompt_df[['mr', 'candidate_pred', 'candidate_prob']].to_csv('viggo_test_5_shuffle_output - cc-clean.csv')"]},{"cell_type":"code","execution_count":null,"id":"aef98f22-e12b-4253-b892-cf1815f9ee37","metadata":{"id":"aef98f22-e12b-4253-b892-cf1815f9ee37"},"outputs":[],"source":["# prompt_df = pd.read_csv('../test_files/viggo_test_5_shuffle_output - cc-clean.csv')"]},{"cell_type":"code","execution_count":null,"id":"85a342f8-c0af-4770-a1aa-e607c955992c","metadata":{"id":"85a342f8-c0af-4770-a1aa-e607c955992c"},"outputs":[],"source":["# prompt_df[prompt_df.candidate=='Do you like playing Diablo II?']"]},{"cell_type":"code","execution_count":null,"id":"0532280e-4a65-46ad-bce2-db3d446d077b","metadata":{"id":"0532280e-4a65-46ad-bce2-db3d446d077b"},"outputs":[],"source":["# prompt_df = pd.read_csv('../test_files/viggo_test_custom25_output - viggo_test_custom_output.csv')\n"]},{"cell_type":"code","execution_count":null,"id":"2bd79960-9896-473d-9b64-1eefe4a07608","metadata":{"id":"2bd79960-9896-473d-9b64-1eefe4a07608"},"outputs":[],"source":["# prompt_df[['mr', 'generated']].info()"]},{"cell_type":"code","execution_count":null,"id":"145982cc-32a8-48b4-be68-66ed0a8e160e","metadata":{"id":"145982cc-32a8-48b4-be68-66ed0a8e160e"},"outputs":[],"source":["# prompt_df = pd.read_csv('../test_files/viggo_test_custom25_output - viggo_test_custom_output.csv')\n","\n","# prompt_df = prompt_df[['mr','generated']].dropna().reset_index(drop=True)\n","\n","\n","\n","# def predict(col):\n","#     df = prompt_df.copy()\n","#     tokenized_data = tokenizer(df[col].values.tolist())\n","#     tokenized_data = data_collator(tokenized_data)\n","    \n","    \n","#     out = model(tokenized_data, mode = 'Test')\n","#     probs = torch.nn.functional.softmax(out, dim = 1).detach().cpu().numpy()\n","#     pred = out.argmax(dim = 1).cpu().numpy()\n","#     pred = [train_dataset.itos[i] for i in pred]\n","#     probs = [i.max() for i in probs]\n","\n","#     df[f'{col}_pred'] = 1\n","#     df[f'{col}_prob'] = 1\n","#     for i in range(len(df)):\n","#         df.loc[i,f'{col}_pred'] = pred[i]\n","#         df.loc[i,f'{col}_prob'] = probs[i]\n","#     return df\n","\n","# prompt_df = predict('generated')\n","# #prompt_df = predict('best_ref')\n","# prompt_df['actual'] = prompt_df['mr'].apply(lambda x: x.split('|')[0].split('=')[0].strip())\n","\n","# print(prompt_df.apply(lambda x: 1 if x['candidate_pred'] == x['actual'] else 0,axis=1).value_counts())\n","\n","# prompt_df.to_csv('../test_files/viggo_test_custom25_output - viggo_test_custom_output.csv')"]},{"cell_type":"code","execution_count":null,"id":"37736de9-ef10-464a-bbde-0c970b72e6ea","metadata":{"id":"37736de9-ef10-464a-bbde-0c970b72e6ea"},"outputs":[],"source":["prompt_df = pd.read_csv('../test_files/viggo_test_custom25_output - viggo_test_custom_output.csv')\n","\n","prompt_df = prompt_df[['mr','generated']].dropna().reset_index(drop=True)\n","\n","\n","\n","def predict(col):\n","    df = prompt_df.copy()\n","    tokenized_data = tokenizer(df[col].values.tolist())\n","    tokenized_data = data_collator(tokenized_data)\n","    print(tokenized_data)\n","    outs = []\n","    \n","    for i in range(0,len(tokenized_data['input_ids']), 100):\n","        out = model(tokenized_data['input_ids'][i:i+100], mode = 'Test')\n","        outs.append(out)\n","    \n","    \n","    probs = torch.nn.functional.softmax(out, dim = 1).detach().cpu().numpy()\n","    pred = out.argmax(dim = 1).cpu().numpy()\n","    pred = [train_dataset.itos[i] for i in pred]\n","    probs = [i.max() for i in probs]\n","\n","    df[f'{col}_pred'] = 1\n","    df[f'{col}_prob'] = 1\n","    for i in range(len(df)):\n","        df.loc[i,f'{col}_pred'] = pred[i]\n","        df.loc[i,f'{col}_prob'] = probs[i]\n","    return df\n","\n","prompt_df = predict('generated')\n","#prompt_df = predict('best_ref')\n","prompt_df['actual'] = prompt_df['mr'].apply(lambda x: x.split('|')[0].split('=')[0].strip())\n","\n","print(prompt_df.apply(lambda x: 1 if x['candidate_pred'] == x['actual'] else 0,axis=1).value_counts())\n","\n","prompt_df.to_csv('../test_files/viggo_test_custom25_output - viggo_test_custom_output.csv')"]},{"cell_type":"code","execution_count":null,"id":"dee8581c-b932-4414-98f7-859f4e4738b8","metadata":{"id":"dee8581c-b932-4414-98f7-859f4e4738b8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"81c28da4-d764-44e1-8182-dca085df2d55","metadata":{"id":"81c28da4-d764-44e1-8182-dca085df2d55"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"9402920d-da63-466e-b38d-1cac81f274c7","metadata":{"id":"9402920d-da63-466e-b38d-1cac81f274c7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"6ec4f82e-013b-499e-9206-c776a339d39a","metadata":{"id":"6ec4f82e-013b-499e-9206-c776a339d39a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"25d19a62-200d-4703-9efe-170b3c40250e","metadata":{"id":"25d19a62-200d-4703-9efe-170b3c40250e"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"17TYBX9NqCY_w6HDo_VnbitRb2Dr9jyDI","timestamp":1679788203462}],"machine_shape":"hm"},"gpuClass":"premium","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3b49d3413ec344488a056854045f62f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c656ce6915f42f5be9104709d75954e","IPY_MODEL_4cc837a2eb5d431c8808ff33ed4fcb67","IPY_MODEL_11849b4f84f344b99b52dffaf53c81fc"],"layout":"IPY_MODEL_fcf5589714fd4cce85af3a5c5dd26f00"}},"0c656ce6915f42f5be9104709d75954e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8af565791afd4645922bda5e017e7d17","placeholder":"​","style":"IPY_MODEL_921d9804aaaf467dbd6ec9deb85f0077","value":"Downloading pytorch_model.bin: 100%"}},"4cc837a2eb5d431c8808ff33ed4fcb67":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a314bbe505d3439e8dba60608c01e3a1","max":435779157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2196568f99694a44912258605d57d4ed","value":435779157}},"11849b4f84f344b99b52dffaf53c81fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_975b2c13c15f44b58727f769b42dce45","placeholder":"​","style":"IPY_MODEL_ce8f34cfd39b43eeb871921587ccc378","value":" 436M/436M [00:02&lt;00:00, 205MB/s]"}},"fcf5589714fd4cce85af3a5c5dd26f00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8af565791afd4645922bda5e017e7d17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"921d9804aaaf467dbd6ec9deb85f0077":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a314bbe505d3439e8dba60608c01e3a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2196568f99694a44912258605d57d4ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"975b2c13c15f44b58727f769b42dce45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce8f34cfd39b43eeb871921587ccc378":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}